{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we'll start playing around with different classifiers and try to submit a prediction to the Kaggle competition. Nothing fancy, just so you won't try to do that the first time in the last day before the due date.\n",
    "\n",
    "This notebook is designed to assist you in playing around with those classifiers, though most of the code is already in the homework assignment writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "from __future__ import division # For python 2.*\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mltools as ml\n",
    "\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "X = np.genfromtxt('data/X_train.txt', delimiter=None)\n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=None)\n",
    "\n",
    "# The test data\n",
    "Xte = np.genfromtxt('data/X_test.txt', delimiter=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All your work should be done on the training data set. To be able to make educated decisions on which classifier you're going to use, you should split it into train and validation data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xva, Ytr, Yva = ml.splitData(X, Y)\n",
    "Xtr, Ytr = ml.shuffleData(Xtr, Ytr)\n",
    "\n",
    "# Taking a subsample of the data so that trains faster.  You should train on whole data for homework and Kaggle.\n",
    "Xt, Yt = Xtr[:4000], Ytr[:4000] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World of Classifiers\n",
    "Time to start doing some classifications. We'll use all those you are required to from the assignment on the data. We'll skip the KNN one, if you want a reminder on how to use them see previous discussions.\n",
    "\n",
    "### IMPORTANT NOTE!!! \n",
    "For the Kaggle dataset you need to submit probabilities and not just class predictions. Don't worry, you don't need to code that, just use the predictSoft() function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decision tree classifier has minLeaf and maxDepth parameters. You should know what it means by now.\n",
    "learner = ml.dtree.treeClassify(Xt, Yt, minLeaf=25, maxDepth=15)\n",
    "\n",
    "# Prediction\n",
    "probs = learner.predictSoft(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictSoft method returns an $M \\times C$ table in which for each point you have the proability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.75      ,  0.25      ],\n",
       "       [ 0.90384615,  0.09615385],\n",
       "       [ 0.92424242,  0.07575758],\n",
       "       ..., \n",
       "       [ 0.94827586,  0.05172414],\n",
       "       [ 0.7175    ,  0.2825    ],\n",
       "       [ 0.98214286,  0.01785714]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the AUC for both the training and validation data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Train AUC: 0.8066\n",
      " Validation AUC: 0.6209\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC', learner.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', learner.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with different parameters to see how AUC changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing decision tree\n",
    "Funny enough, whoever wrote the decision tree classifier provided a printing mechanism. However, it only works up to depth 2, so not very useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  if x[0] < 237.245000:\n",
      "    if x[2] < 242.215000:\n",
      "      Predict [ 0.45227996  0.54772004]\n",
      "    else:\n",
      "      Predict [ 0.69883041  0.30116959]\n",
      "  else:\n",
      "    if x[0] < 249.030000:\n",
      "      Predict [ 0.69539337  0.30460663]\n",
      "    else:\n",
      "      Predict [ 0.79384134  0.20615866]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner = ml.dtree.treeClassify()\n",
    "learner.train(Xt, Yt, maxDepth=2)\n",
    "print (learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 14) (200000, 14)\n"
     ]
    }
   ],
   "source": [
    "# Scaling the data\n",
    "XtrP, params = ml.rescale(Xt)\n",
    "XteP,_    = ml.rescale(Xte, params)\n",
    "\n",
    "print(XtrP.shape, XteP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note that we do not need to scale the data for decision tree. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dheer\\Documents\\workspace\\ml-discussions\\week5\\mltools\\base.py:97: RuntimeWarning: divide by zero encountered in log\n",
      "  return - np.mean( np.log( P[ np.arange(M), Y ] ) ) # evaluate\n",
      "C:\\Users\\dheer\\Documents\\workspace\\ml-discussions\\week5\\mltools\\linearC.py:134: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  done = (it > stopIter) or ( (it>1) and (abs(Jsur[-1]-Jsur[-2])<stopTol) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7794684   0.2205316 ]\n",
      " [ 0.56946355  0.43053645]\n",
      " [ 0.81647372  0.18352628]\n",
      " [ 0.79368576  0.20631424]\n",
      " [ 0.72786685  0.27213315]]\n"
     ]
    }
   ],
   "source": [
    "## Linear models:\n",
    "learner = ml.linearC.linearClassify()\n",
    "learner.train(XtrP, Yt, initStep=0.5, stopTol=1e-6, stopIter=100)\n",
    "\n",
    "probs = learner.predictSoft(XteP)\n",
    "print(probs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the AUC IS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Train AUC: 0.6472\n",
      " Validation AUC: 0.5763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dheer\\Documents\\workspace\\ml-discussions\\week5\\mltools\\linearC.py:82: RuntimeWarning: overflow encountered in exp\n",
      "  prob = np.exp(resp)\n",
      "C:\\Users\\dheer\\Documents\\workspace\\ml-discussions\\week5\\mltools\\linearC.py:84: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prob /= prob + 1.0       # logistic transform (binary classification; C=1)\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC',learner.auc(XtrP, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', learner.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why we're using a validation data set. We can see already that for THIS specific configuration the decision tree is much better. It is very likely that it'll be better on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "Yeah, even that is given to you in the mltools package. We'll use it in our examples. Having said that, if you want to use some more fancy packages you are more than welcome to do that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ml.nnet.nnetClassify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we construct the classifier, we can define the sizes of its layers and initialize their values with \"init_weights\".\n",
    "\n",
    "Definition of nn.init_weights:\n",
    "\n",
    "        nn.init_weights(self, sizes, init, X, Y)\n",
    "\n",
    "From the method description: sizes = [Ninput, N1, N2, ... , Noutput], where Ninput = # of input features, and Nouput = # classes\n",
    "\n",
    "Training the model using gradient descent, we can track the surrogate loss (here, MSE loss on the output vector, compared to a 1-of-K representation of the class), as well as the 0/1 classification loss (error rate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "layers[-1] must equal the number of classes in Y, or 1 for binary Y",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-f8f31dba052e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'random'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopTol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstepsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopIter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\workspace\\ml-discussions\\week5\\mltools\\nnet.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, Y, init, stepsize, stopTol, stopIter)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# and (self.wts[-1].shape[0]!=1 or len(self.classes)!=2):\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layers[-1] must equal the number of classes in Y, or 1 for binary Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: layers[-1] must equal the number of classes in Y, or 1 for binary Y"
     ]
    }
   ],
   "source": [
    "nn.init_weights([14, 5, 3], 'random', Xt, Yt)\n",
    "nn.train(Xt, Yt, stopTol=1e-8, stepsize=.25, stopIter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHAT DID WE DO WRONG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1 : Jsur = 0.4792564751296402, J01 = 0.37075\n",
      "it 2 : Jsur = 0.45087445509009433, J01 = 0.338\n",
      "it 4 : Jsur = 0.44029500022370305, J01 = 0.338\n",
      "it 8 : Jsur = 0.43575700229893755, J01 = 0.338\n",
      "it 16 : Jsur = 0.43453152938164963, J01 = 0.338\n",
      "it 32 : Jsur = 0.43421584910250005, J01 = 0.338\n"
     ]
    }
   ],
   "source": [
    "# Need to specify the right number of input and output layers.\n",
    "nn.init_weights([Xt.shape[1], 5, len(np.unique(Yt))], 'random', Xt, Yt)\n",
    "nn.train(Xt, Yt, stopTol=1e-8, stepsize=.25, stopIter=50)  # Really small stopIter so it will stop fast :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Train AUC: 0.5951\n",
      " Validation AUC: 0.5874\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC',nn.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', nn.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC results are bad because we just used a lame configuration of the NN. NN can be engineered until your last day, but some things should make sense to you.\n",
    "\n",
    "One example is the option to change the activation function. This is the function that is in the inner layers. By default the code comes with the tanh, but the logistic (sigmoid) is also coded in and you can just specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dheer\\Documents\\workspace\\ml-discussions\\week5\\mltools\\nnet.py:218: RuntimeWarning: overflow encountered in exp\n",
      "  self.Sig = lambda z: twod(1 / (1 + np.exp(-z)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 1 : Jsur = 0.4458566202394441, J01 = 0.338\n",
      "it 2 : Jsur = 0.43532519882944687, J01 = 0.338\n",
      "it 4 : Jsur = 0.4346481566453837, J01 = 0.338\n",
      "it 8 : Jsur = 0.43420320182100297, J01 = 0.339\n",
      "it 16 : Jsur = 0.43367241889141583, J01 = 0.334\n",
      "it 32 : Jsur = 0.43351585672942405, J01 = 0.33125\n",
      "it 64 : Jsur = 0.43510633570074697, J01 = 0.3365\n",
      "      Train AUC: 0.5842\n",
      " Validation AUC: 0.5738\n"
     ]
    }
   ],
   "source": [
    "nn.setActivation('logistic')\n",
    "\n",
    "nn.train(Xt, Yt, stopTol=1e-8, stepsize=.25, stopIter=100)\n",
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC',nn.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', nn.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own activation function\n",
    "\n",
    "Not suprisingly, you can also provide a custom activation function. Note that for the last layer you will probably always want the sigmoid function, so only change the inner layers ones.\n",
    "\n",
    "The function definition is this:\n",
    "\n",
    "    setActivation(self, method, sig=None, d_sig=None, sig_0=None, d_sig_0=None)\n",
    "    \n",
    "You can call it with method='custom' and then specify both sig and d_sig. (the '0' ones are for the last layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a dummy activation method (f(x) = x)\n",
    "sig = lambda z: np.atleast_2d(z)\n",
    "dsig = lambda z: np.atleast_2d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ml.nnet.nnetClassify()\n",
    "nn.init_weights([Xt.shape[1], 5, len(np.unique(Yt))], 'random', Xt, Yt)\n",
    "\n",
    "nn.setActivation('custom', sig, dsig)\n",
    "\n",
    "nn.train(Xt, Yt, stopTol=1e-8, stepsize=.25, stopIter=100)\n",
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC',nn.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', nn.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train AUC: 0.5198 <br>\n",
    " Validation AUC: 0.4999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "We've learn that one way of guessing how well we're doing with different model parameters is to plot the train and validation errors as a function of that paramter (e.g, k in the KNN of degree in the linear classifier and regression).\n",
    "\n",
    "Now it seems like there could be more parameters involved? One example is the degree and the regularizer value (see. HW assignment for more examples).\n",
    "\n",
    "When it's two features you can simple use heatmaps. The X-axis and Y-axis represent the parameters and the \"heat\" is the validation/train error as a \"third\" dimension\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a dummy function to show that. Let's assume we have two parameters p1 and p2 and the prediction accuracy is p1 + p2 (yup, that stupid). In the HW assignment it's actually the auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.arange(5)\n",
    "p2 = np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = np.zeros([p1.shape[0], p2.shape[0]])\n",
    "for i in range(p1.shape[0]):\n",
    "    for j in range(p2.shape[0]):\n",
    "        auc[i][j] = p1[i] + p2[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "cax = ax.matshow(auc)\n",
    "f.colorbar(cax)\n",
    "\n",
    "ax.set_xticks(p1)\n",
    "ax.set_xticklabels(['%d' % p for p in p1])\n",
    "\n",
    "ax.set_yticks(p2)\n",
    "ax.set_yticklabels(['%d' % p for p in p2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> For homework: <br> <br>\n",
    "\n",
    "x and y will be hyperparameters of the model <br>\n",
    "f will be some performance metric (error, AUC etc.) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Predictions\n",
    "\n",
    "Let's assume that the last classifier we ran was the best one (after we used all that we know to verify it is the best one including that plot from the previous block). Now let's run it on the test and create a file that can be submitted.\n",
    "\n",
    "Each line in the file is a point id and the probability of P(Y=1). There's also a header file. Here's how you can create it simply from the probs matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data for submission by taking the P(Y=1) column from probs and just add a running index as the first column.\n",
    "Y_sub = np.vstack([np.arange(Xte.shape[0]), probs[:, 1]]).T\n",
    "\n",
    "# We specify the header (ID, Prob1) and also specify the comments as '' so the header won't be commented out with\n",
    "# the # sign.\n",
    "np.savetxt('data/Y_sub.txt', Y_sub, '%d, %.5f', header='ID,Prob1', comments='', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
